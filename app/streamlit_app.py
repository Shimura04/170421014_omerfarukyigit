# Intent SÄ±nÄ±flandÄ±rma (Embedding + LogisticRegression)
import streamlit as st
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
import joblib

from dotenv import load_dotenv
import os

# LangChain + Gemini + Llama
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# BaÅŸlÄ±k
st.set_page_config(page_title="TÄ±bbi Asistan Chatbot", layout="wide")

# Sol Ã¼st kÃ¶ÅŸe baÅŸlÄ±k 
st.sidebar.markdown("""
    <style>
    .sidebar-title {
        font-size: 20px;
        font-weight: bold;
        padding: 10px 0 5px 5px;
        color: #ffffff;
    }

    section[data-testid="stSidebar"]::before {
        content: " ";
        display: block;
        margin-bottom: 10px;
        border-bottom: 1px solid #444;
    }
    </style>
    <div class="sidebar-title">ğŸ¥ TÄ±bbi Asistan</div>
""", unsafe_allow_html=True)

# Session state
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "model_comparisons" not in st.session_state:
    st.session_state.model_comparisons = []

if "show_comparison_analysis" not in st.session_state:
    st.session_state.show_comparison_analysis = False

import os
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Intent Model ve Label Encoder YÃ¼kleme
model_path = os.path.join(BASE_DIR, "..", "data", "medical_intent_classifier.joblib")
encoder_path = os.path.join(BASE_DIR, "..", "data", "medical_label_encoder.joblib")

if not os.path.exists(model_path):
    try:
        st.info("ğŸ”„ Model dosyasÄ± bulunamadÄ±, yeni model eÄŸitiliyor...")
        
        # Dataset dosyasÄ±nÄ±n varlÄ±ÄŸÄ±nÄ± kontrol et
        dataset_path = os.path.join(BASE_DIR, "..", "medibot_dataset_complete.xlsx")
        if not os.path.exists(dataset_path):
            st.error(f"âŒ Dataset dosyasÄ± bulunamadÄ±: {dataset_path}")
            st.stop()
        
        # Data klasÃ¶rÃ¼nÃ¼ oluÅŸtur (yoksa)
        data_dir = os.path.join(BASE_DIR, "..", "data")
        os.makedirs(data_dir, exist_ok=True)
        
        # TamamlanmÄ±ÅŸ dataset'i yÃ¼kle
        st.write(f"ğŸ“‚ Dataset yÃ¼kleniyor: {dataset_path}")
        df = pd.read_excel(dataset_path)
        df = df.dropna()
        
        if df.empty:
            st.error("âŒ Dataset boÅŸ!")
            st.stop()
        
        # SÃ¼tun kontrolÃ¼
        required_columns = ['user_message', 'balanced_intent']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            st.error(f"âŒ Dataset'te eksik sÃ¼tunlar: {missing_columns}")
            st.write("Mevcut sÃ¼tunlar:", list(df.columns))
            st.stop()
        
        # Dengeli veri setinde balanced_intent kullan
        st.write(f"ğŸ“Š Dataset yÃ¼klendi: {len(df)} Ã¶rnek")
        st.write(f"ğŸ·ï¸ Kategori sayÄ±sÄ±: {len(df['balanced_intent'].unique())} adet")
        
        # Dengeli intent'leri kullan
        df['simplified_intent'] = df['balanced_intent']
        
        # Intent daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
        intent_counts = df['simplified_intent'].value_counts()
        st.write("ğŸ“ˆ Dataset DaÄŸÄ±lÄ±mÄ±:")
        st.dataframe(intent_counts)

        # Embedding modeli yÃ¼kle
        st.write("ğŸ¤– Model eÄŸitimi iÃ§in embedding modeli hazÄ±rlanÄ±yor...")
        import torch
        
        # Device belirleme
        if torch.backends.mps.is_available():
            device = "mps"
        elif torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
        
        try:
            embedding_model_training = SentenceTransformer(
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                device=device
            )
        except:
            st.warning("âš ï¸ Ana model yÃ¼klenemedi, alternatif model kullanÄ±lÄ±yor...")
            embedding_model_training = SentenceTransformer(
                "sentence-transformers/all-MiniLM-L6-v2",
                device="cpu"
            )
        
        # Embedding oluÅŸtur
        st.write("ğŸ”„ Embeddings oluÅŸturuluyor...")
        X = embedding_model_training.encode(df["user_message"].tolist())
        
        # Label encoder
        st.write("ğŸ·ï¸ Label encoding...")
        le = LabelEncoder()
        y = le.fit_transform(df["simplified_intent"])

        # Model EÄŸitim
        st.write("âš™ï¸ Model eÄŸitiliyor...")
        clf = LogisticRegression(max_iter=1000, random_state=42, C=1.0, class_weight='balanced')
        clf.fit(X, y)
        
        # Model performansÄ±nÄ± test et
        from sklearn.model_selection import cross_val_score
        scores = cross_val_score(clf, X, y, cv=5)
        st.success(f"âœ… Model eÄŸitildi! DoÄŸruluk: {scores.mean():.3f}")

        # Kaydet
        st.write("ğŸ’¾ Model kaydediliyor...")
        joblib.dump(clf, model_path)
        joblib.dump(le, encoder_path)
        st.success(f"âœ… Model baÅŸarÄ±yla kaydedildi!")
        st.success(f"ğŸ“ Model dosyasÄ±: {model_path}")
        st.success(f"ğŸ“ Encoder dosyasÄ±: {encoder_path}")
        
    except Exception as e:
        st.error(f"âŒ Model eÄŸitimi sÄ±rasÄ±nda hata: {str(e)}")
        st.error(f"Hata tipi: {type(e).__name__}")
        import traceback
        st.code(traceback.format_exc())
        st.stop()
else:
    try:
        # Mevcut modeli yÃ¼kle
        clf = joblib.load(model_path)
        le = joblib.load(encoder_path)
        st.success("âœ… Mevcut model baÅŸarÄ±yla yÃ¼klendi!")
    except Exception as e:
        st.error(f"âŒ Model yÃ¼klenirken hata: {str(e)}")
        st.stop()

# Embedding modeli gÃ¼venli yÃ¼kleme
@st.cache_resource
def load_embedding_model():
    import torch
    try:
        # Device belirleme (Apple Silicon Mac iÃ§in)
        if torch.backends.mps.is_available():
            device = "mps"
        elif torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
        
        st.info(f"ğŸ¤– Embedding modeli yÃ¼kleniyor... (Device: {device})")
        
        # TÃ¼rkÃ§e destekli model - device belirtme
        model = SentenceTransformer(
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            device=device
        )
        st.success("âœ… Embedding modeli baÅŸarÄ±yla yÃ¼klendi!")
        return model
        
    except Exception as e:
        st.warning(f"âš ï¸ Ana model yÃ¼klenemedi: {e}")
        st.info("ğŸ”„ Alternatif model deneniyor...")
        
        # Fallback: Daha basit model
        try:
            model = SentenceTransformer(
                "sentence-transformers/all-MiniLM-L6-v2",
                device="cpu"  # CPU'da Ã§alÄ±ÅŸtÄ±r
            )
            st.success("âœ… Alternatif embedding modeli yÃ¼klendi!")
            return model
        except Exception as e2:
            st.error(f"âŒ HiÃ§bir embedding model yÃ¼klenemedi: {e2}")
            return None

# Global embedding model
embedding_model = load_embedding_model()

# Intent Tahmin Fonksiyonu (GÃ¼Ã§lendirilmiÅŸ)
def predict_intent(text):
    if embedding_model is None:
        # Fallback: gÃ¼venilir deÄŸil
        return "non_medical", 0.1
    
    try:
        # Metni encode et
        vec = embedding_model.encode([text])
        
        # Tahmin yap
        pred = clf.predict(vec)
        probabilities = clf.predict_proba(vec)[0]
        confidence = probabilities.max()
        intent = le.inverse_transform(pred)[0]
        
        # En yÃ¼ksek 2 probability'yi karÅŸÄ±laÅŸtÄ±r (belirsizlik kontrolÃ¼)
        sorted_probs = sorted(probabilities, reverse=True)
        if len(sorted_probs) > 1:
            uncertainty = sorted_probs[0] - sorted_probs[1]
            # EÄŸer en yÃ¼ksek ile ikinci yÃ¼ksek arasÄ±nda az fark varsa gÃ¼ven azalt
            if uncertainty < 0.2:
                confidence *= 0.8
        
        return intent, confidence
        
    except Exception as e:
        print(f"Intent tahmin hatasÄ±: {e}")
        return "non_medical", 0.1

# AkÄ±llÄ± acil durum kontrolÃ¼ - Dengeli kategorilere gÃ¶re
def is_emergency(intent, user_message="", urgency_level="UNKNOWN"):
    # GERÃ‡EK ACÄ°L DURUMLAR (112'lik)
    true_emergency_keywords = [
        'nefes alamÄ±yorum', 'gÃ¶ÄŸsÃ¼mde aÄŸrÄ±', 'bayÄ±lÄ±yorum', 'bayÄ±ldÄ±m',
        'bilinÃ§ kaybÄ±', 'felÃ§ geÃ§irdim', 'kalbim duruyor', 'ÅŸiddetli gÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ±',
        'kalp krizi', 'nefes darlÄ±ÄŸÄ± Ã§ok ÅŸiddetli', 'Ã¶ldÃ¼rÃ¼cÃ¼ aÄŸrÄ±',
        'intihar etmek', 'kendimi Ã¶ldÃ¼rmek', 'Ã§ok yÃ¼ksek ateÅŸ 40',
        'ÅŸuur kaybÄ±', 'komada', 'kanama durmuyor', 'Ã§ok fazla kan kaybÄ±',
        'zehirlendim', 'overdoz', 'aÅŸÄ±rÄ± doz'
    ]
    
    # SÃ¶zcÃ¼k kontrolÃ¼ (TÃ¼rkÃ§e kÃ¼Ã§Ã¼k harf)
    message_lower = user_message.lower()
    has_true_emergency = any(keyword in message_lower for keyword in true_emergency_keywords)
    
    # Sadece GERÃ‡EK emergency intent'i VE kritik kelimeler varsa acil
    is_emergency_intent = intent == 'emergency'
    is_critical_urgency = urgency_level == "CRITICAL"
    
    # Ã‡OCUK DOKTORU durumlarÄ± acil deÄŸil
    child_non_emergency = any(word in message_lower for word in [
        'diken battÄ±', 'Ã§ocuÄŸ', 'kaÅŸÄ±ntÄ±', 'morarma', 'kÃ¼Ã§Ã¼k yara'
    ])
    
    # GerÃ§ek acil: (Emergency intent VE kritik kelime) VEYA kritik urgency
    return (is_emergency_intent and has_true_emergency) or is_critical_urgency

# TÄ±bbi PDF iÃ§in RAG (EÄŸer tÄ±bbi dÃ¶kÃ¼manÄ±nÄ±z varsa)
faiss_index_path = os.path.join(BASE_DIR, "..", "data", "medical_faiss_index")
embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=os.getenv("GEMINI_API_KEY"))  # Gemini embedding kullan

if os.path.exists(os.path.join(faiss_index_path, "index.faiss")):
    vectorstore = FAISS.load_local(
        folder_path=faiss_index_path,
        embeddings=embedding,
        allow_dangerous_deserialization=True
    )
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
else:
    # EÄŸer tÄ±bbi PDF'niz varsa buraya ekleyin
    pdf_path = os.path.join(BASE_DIR, "..", "data", "medical_guide.pdf")  # PDF dosya yolunuz
    if os.path.exists(pdf_path):
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)
        chunks = splitter.split_documents(docs)
        vectorstore = FAISS.from_documents(chunks, embedding)
        vectorstore.save_local(faiss_index_path)
        retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
    else:
        retriever = None

# LLM Modeller
llm_gemini = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash-latest",
    temperature=0.3,
    max_tokens=500,
    google_api_key=os.getenv("GEMINI_API_KEY"),
    convert_system_message_to_human=True
)

# Llama Model (Local - Ollama)
llm_llama = ChatOllama(
    model="llama3.2:3b",
    temperature=0.3,
    num_predict=500,
)

# TÄ±bbi Prompt
system_prompt = (
    "Sen deneyimli bir tÄ±bbi asistansÄ±n. KullanÄ±cÄ±larÄ±n saÄŸlÄ±k sorularÄ±na yardÄ±mcÄ± oluyorsun. "
    "Ã–NEMLÄ° UYARILAR:\n"
    "1. Kesin teÅŸhis koymayÄ±n, sadece genel bilgi verin\n"
    "2. Acil durumlarda mutlaka doktora baÅŸvurmasÄ±nÄ± sÃ¶yleyin\n"
    "3. Ä°laÃ§ Ã¶nerisi yapmayÄ±n, sadece genel tavsiyelerde bulunun\n"
    "4. CevaplarÄ±nÄ±zÄ± kÄ±sa ve anlaÅŸÄ±lÄ±r tutun\n\n"
    "Verilen iÃ§erik: {context}"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}")
])

# Zincir
qa_chain_gemini = create_stuff_documents_chain(llm_gemini, prompt)

# RAG zinciri
def run_rag_chain(question, model_choice):
    # Model seÃ§imine gÃ¶re LLM belirle
    if "Llama" in model_choice:
        llm = llm_llama
        qa_chain = create_stuff_documents_chain(llm, prompt)
    else:  # Gemini
        llm = llm_gemini
        qa_chain = qa_chain_gemini
    
    if retriever:
        chain = create_retrieval_chain(retriever, qa_chain)
        result = chain.invoke({"input": question})
        return result.get("answer")
    else:
        # RAG olmadan direkt LLM
        if "Llama" in model_choice:
            # Llama iÃ§in sistem promptu dahil etme
            full_prompt = f"""Sen deneyimli bir tÄ±bbi asistansÄ±n. KullanÄ±cÄ±larÄ±n saÄŸlÄ±k sorularÄ±na yardÄ±mcÄ± oluyorsun.

Ã–NEMLÄ° UYARILAR:
1. Kesin teÅŸhis koymayÄ±n, sadece genel bilgi verin
2. Acil durumlarda mutlaka doktora baÅŸvurmasÄ±nÄ± sÃ¶yleyin
3. Ä°laÃ§ Ã¶nerisi yapmayÄ±n, sadece genel tavsiyelerde bulunun
4. CevaplarÄ±nÄ±zÄ± kÄ±sa ve anlaÅŸÄ±lÄ±r tutun

Soru: {question}

Cevap:"""
            response = llm.invoke(full_prompt)
            return response.content
        else:  # Gemini
            return llm.invoke(question).content

# Sidebar ayarlarÄ±
with st.sidebar:
    st.header("âš™ï¸ Ayarlar")
    model_choice = st.selectbox("ğŸ¤– Model SeÃ§imi:", [
        "Gemini-1.5-flash", 
        "Llama-3.2-3B (Local)",
        "KarÅŸÄ±laÅŸtÄ±rma (Her Ä°kisi)"
    ])
    
    # Model karÅŸÄ±laÅŸtÄ±rma Ã¶zelliÄŸi
    if model_choice == "KarÅŸÄ±laÅŸtÄ±rma (Her Ä°kisi)":
        st.info("ğŸ”„ Gemini ve Llama modelleri karÅŸÄ±laÅŸtÄ±rÄ±lacak!")
    elif model_choice == "Llama-3.2-3B (Local)":
        st.success("ğŸ¦™ Local Llama modeli kullanÄ±lÄ±yor - API key gerekmez!")
    
    # Acil durum uyarÄ±sÄ±
    st.error("""
    ğŸš¨ **ACÄ°L DURUM UYARISI**
    Bu chatbot tÄ±bbi tavsiye vermez!
    Acil durumlar iÃ§in:
    ğŸ“ 112 - Ambulans
    ğŸ¥ En yakÄ±n hastaneye gidin
    """)
    
    if st.button("ğŸ—‘ï¸ GeÃ§miÅŸi Temizle"):
        st.session_state.chat_history = []
        st.rerun()
    
    # Model karÅŸÄ±laÅŸtÄ±rmasÄ± geÃ§miÅŸi
    if len(st.session_state.model_comparisons) > 0:
        st.markdown("---")
        st.subheader("ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rmasÄ±")
        st.write(f"Toplam karÅŸÄ±laÅŸtÄ±rma: {len(st.session_state.model_comparisons)}")
        
        if st.button("ğŸ“ˆ KarÅŸÄ±laÅŸtÄ±rma Analizi"):
            st.session_state.show_comparison_analysis = True
        
        if st.button("ğŸ—‘ï¸ KarÅŸÄ±laÅŸtÄ±rmalarÄ± Temizle"):
            st.session_state.model_comparisons = []
            st.rerun()

# Ana baÅŸlÄ±k
st.title("ğŸ¥ TÄ±bbi Asistan Chatbot")
st.markdown("*SaÄŸlÄ±k sorularÄ±nÄ±zda size yardÄ±mcÄ± olmak iÃ§in buradayÄ±m. Acil durumlar iÃ§in mutlaka 112'yi arayÄ±n!*")

# GeÃ§miÅŸi gÃ¶ster
if st.session_state.chat_history:
    st.markdown("## ğŸ’¬ Sohbet GeÃ§miÅŸi")
    for i, (q, a, m) in enumerate(st.session_state.chat_history):
        st.markdown(f"**Soru {i+1}:** {q}")
        st.markdown(f"**YanÄ±t ({m}):** {a}")
        st.markdown("---")

# KullanÄ±cÄ± input
user_input = st.text_input("ğŸ’­ SaÄŸlÄ±k sorunuzu yazÄ±n:", placeholder="Ã–rn: BaÅŸÄ±m aÄŸrÄ±yor ne yapmalÄ±yÄ±m?")
st.button("ğŸ“¤ GÃ¶nder", use_container_width=True)

if user_input:
    with st.spinner("ğŸ¤” DÃ¼ÅŸÃ¼nÃ¼yor..."):
        # Intent tahmin et
        intent, confidence = predict_intent(user_input)
        
        # Sadece tÄ±bbi kategoriler tanÄ±mla (greeting ve farewell hariÃ§)
        medical_categories = {
            'general_consultation', 'emergency', 'ophthalmology', 'digestive', 
            'orthopedic', 'otolaryngology', 'respiratory', 'pediatric', 
            'neurology', 'cardiovascular', 'urology', 'dermatology', 
            'geriatric', 'endocrine', 'mental_health', 'medication', 
            'womens_health', 'pain_management'
        }
        
        # SIKI Confidence threshold kontrolÃ¼ ve kategori filtreleme
        # Sadece yÃ¼ksek gÃ¼venle ve tÄ±bbi kategorilerde cevap ver
        if confidence < 0.6 or intent == "non_medical" or (intent not in medical_categories and intent not in ['greeting', 'farewell']):
            answer = f"""
            ğŸ¥ **ÃœzgÃ¼nÃ¼m, bu soruyu yanÄ±tlayamÄ±yorum.**
            
            Ben sadece aÅŸaÄŸÄ±daki tÄ±bbi konularda yardÄ±mcÄ± olabilirim:
            
            **ğŸ©º TÄ±bbi UzmanlÄ±k AlanlarÄ±m:**
            ğŸ«€ Kardiyoloji (Kalp ve damar hastalÄ±klarÄ±)
            ğŸ§  NÃ¶roloji (Sinir sistemi hastalÄ±klarÄ±) 
            ğŸ‘ï¸ GÃ¶z hastalÄ±klarÄ± (Oftalmoloji)
            ğŸ¦´ Ortopedi (Kemik ve eklem hastalÄ±klarÄ±)
            ğŸ‘‚ Kulak-Burun-BoÄŸaz hastalÄ±klarÄ±
            ğŸ« Solunum yolu hastalÄ±klarÄ±
            ğŸ‘¶ Ã‡ocuk hastalÄ±klarÄ± (Pediatri)
            ğŸ½ï¸ Sindirim sistemi hastalÄ±klarÄ±
            ğŸ©º Genel tÄ±bbi danÄ±ÅŸmanlÄ±k
            ğŸ’Š Ä°laÃ§ bilgileri ve kullanÄ±mÄ±
            ğŸ‘©â€âš•ï¸ KadÄ±n saÄŸlÄ±ÄŸÄ±
            ğŸ§“ Geriatri (YaÅŸlÄ±lÄ±k hastalÄ±klarÄ±)
            ğŸ¥ Acil durumlar
            ğŸ§´ Deri hastalÄ±klarÄ±
            âš–ï¸ Hormon hastalÄ±klarÄ± (Endokrin)
            ğŸ§  Ruh saÄŸlÄ±ÄŸÄ±
            ğŸ’‰ AÄŸrÄ± yÃ¶netimi
            ğŸš½ Ãœroloji (Ä°drar yolu hastalÄ±klarÄ±)
            
            **LÃ¼tfen yukarÄ±daki konulardan biriyle ilgili soru sorun.**
            
            ğŸš¨ **Acil durumda 112'yi arayÄ±n!**
            """
            intent_display = f"KONU DIÅI - Intent: {intent} (GÃ¼ven: {confidence:.2f})"
        
        # EÄŸer intent konu dÄ±ÅŸÄ± ise sabit cevap (dataset'teki gerÃ§ek non_medical intent'i)
        elif intent == "greeting":
            greetings = [
                "Merhaba! Size saÄŸlÄ±k konularÄ±nda nasÄ±l yardÄ±mcÄ± olabilirim? ğŸ¥",
                "Selam! SaÄŸlÄ±k sorunlarÄ±nÄ±zla ilgili sorularÄ±nÄ±zÄ± bekliyorum. ğŸ˜Š",
                "Ä°yi gÃ¼nler! TÄ±bbi konularda size nasÄ±l destek olabilirim? ğŸ©º",
                "HoÅŸ geldiniz! SaÄŸlÄ±ÄŸÄ±nÄ±zla ilgili merak ettiklerinizi sorabilirsiniz. ğŸ’™",
                "Merhaba! Ben tÄ±bbi asistan chatbot'uyum. Size nasÄ±l yardÄ±mcÄ± olabilirim? ğŸ¤–"
            ]
            import random
            answer = random.choice(greetings)
        # Farewell (VedalaÅŸma) mesajlarÄ±  
        elif intent == "farewell":
            farewells = [
                "GÃ¶rÃ¼ÅŸÃ¼rÃ¼z! SaÄŸlÄ±klÄ± gÃ¼nler dilerim. ğŸŒŸ",
                "HoÅŸÃ§a kalÄ±n! Kendinize iyi bakÄ±n. ğŸ’š",
                "GÃ¼le gÃ¼le! BaÅŸka sorularÄ±nÄ±z olursa buradayÄ±m. ğŸ‘‹",
                "Ä°yi gÃ¼nler! SaÄŸlÄ±ÄŸÄ±nÄ±zda kalÄ±n. ğŸ™",
                "Elveda! Her zaman saÄŸlÄ±k konularÄ±nda yardÄ±mcÄ± olmaya hazÄ±rÄ±m. ğŸ˜Š",
                "SaÄŸlÄ±cakla kalÄ±n! Tekrar gÃ¶rÃ¼ÅŸmek dileÄŸiyle. ğŸŒˆ"
            ]
            import random
            answer = random.choice(farewells)
        elif is_emergency(intent, user_input):
            answer = f"""
            ğŸš¨ **ACÄ°L DURUM TESPÄ°T EDÄ°LDÄ°!**
            
            LÃ¼tfen derhal:
            ğŸ“ 112'yi arayÄ±n
            ğŸ¥ En yakÄ±n hastaneye gidin
            
            Bu ciddi bir durum olabilir ve profesyonel tÄ±bbi mÃ¼dahale gerektirir.
            """
        else:
            # Normal tÄ±bbi sorular - Model seÃ§imine gÃ¶re Ã§alÄ±ÅŸtÄ±r
            if model_choice == "KarÅŸÄ±laÅŸtÄ±rma (Her Ä°kisi)":
                # Her iki modeli de Ã§alÄ±ÅŸtÄ±r
                answers = {}
                models_to_test = ["Gemini-1.5-flash", "Llama-3.2-3B (Local)"]
                
                for model in models_to_test:
                    try:
                        current_answer = run_rag_chain(user_input, model)
                        
                        # EÄŸer Ã§ok kÄ±sa cevap geliyorsa, detaylandÄ±r
                        if len(current_answer) < 100:
                            enhanced_prompt = f"""
                            KullanÄ±cÄ±nÄ±n saÄŸlÄ±k sorusu: "{user_input}"
                            Tespit edilen kategori: {intent}
                            
                            Bu saÄŸlÄ±k sorusuna detaylÄ±, faydalÄ± ve empati dolu bir yanÄ±t ver.
                            Genel tavsiyeler, dikkat edilmesi gerekenler ve ne zaman doktora baÅŸvurulmasÄ± gerektiÄŸini belirt.
                            """
                            if "Llama" in model:
                                current_answer = llm_llama.invoke(enhanced_prompt).content
                            else:
                                current_answer = llm_gemini.invoke(enhanced_prompt).content
                        
                        # GÃ¼venlik uyarÄ±sÄ± ekle
                        current_answer += "\n\nâš ï¸ *Bu bilgi genel amaÃ§lÄ±dÄ±r. Kesin teÅŸhis iÃ§in doktora baÅŸvurun.*"
                        answers[model] = current_answer
                        
                    except Exception as e:
                        error_msg = str(e)
                        if "API key" in error_msg.lower() or "unauthorized" in error_msg.lower():
                            answers[model] = f"âŒ API Key HatasÄ±: {model} iÃ§in API anahtarÄ± geÃ§ersiz veya eksik."
                        else:
                            answers[model] = f"âŒ Hata: {model} - {error_msg[:100]}"
                
                # KarÅŸÄ±laÅŸtÄ±rma sonucunu kaydet
                comparison_data = {
                    "timestamp": pd.Timestamp.now(),
                    "question": user_input,
                    "intent": intent,
                    "confidence": confidence,
                    "gemini_answer": answers.get("Gemini-1.5-flash", "Hata"),
                    "llama_answer": answers.get("Llama-3.2-3B (Local)", "Hata")
                }
                st.session_state.model_comparisons.append(comparison_data)
                
                # KarÅŸÄ±laÅŸtÄ±rmalÄ± yanÄ±tÄ± hazÄ±rla
                answer = f"""
                ## ğŸ¤– **Gemini-1.5-flash YanÄ±tÄ±:**
                {answers.get('Gemini-1.5-flash', 'Hata oluÅŸtu')}
                
                ---
                
                ## ğŸ¦™ **Llama-3.2-3B (Local) YanÄ±tÄ±:**
                {answers.get('Llama-3.2-3B (Local)', 'Hata oluÅŸtu')}
                """
                
            else:
                # Tek model Ã§alÄ±ÅŸtÄ±r
                try:
                    answer = run_rag_chain(user_input, model_choice)
                    
                    # EÄŸer Ã§ok kÄ±sa cevap geliyorsa, detaylandÄ±r
                    if len(answer) < 100:
                        enhanced_prompt = f"""
                        KullanÄ±cÄ±nÄ±n saÄŸlÄ±k sorusu: "{user_input}"
                        Tespit edilen kategori: {intent}
                        
                        Bu saÄŸlÄ±k sorusuna detaylÄ±, faydalÄ± ve empati dolu bir yanÄ±t ver.
                        Genel tavsiyeler, dikkat edilmesi gerekenler ve ne zaman doktora baÅŸvurulmasÄ± gerektiÄŸini belirt.
                        """
                        if "Llama" in model_choice:
                            answer = llm_llama.invoke(enhanced_prompt).content
                        else:
                            answer = llm_gemini.invoke(enhanced_prompt).content
                    
                    # GÃ¼venlik uyarÄ±sÄ± ekle
                    answer += "\n\nâš ï¸ *Bu bilgi genel amaÃ§lÄ±dÄ±r. Kesin teÅŸhis iÃ§in doktora baÅŸvurun.*"
                    
                except Exception as e:
                    error_msg = str(e)
                    if "API key" in error_msg.lower() or "unauthorized" in error_msg.lower():
                        answer = f"""
                        âŒ **API Key HatasÄ±**
                        
                        {model_choice} modeli iÃ§in API anahtarÄ± geÃ§ersiz veya eksik.
                        LÃ¼tfen .env dosyasÄ±nda API anahtarÄ±nÄ±zÄ± kontrol edin.
                        
                        GeÃ§ici Ã§Ã¶zÃ¼m:
                        ğŸ”¹ FarklÄ± bir model seÃ§in
                        ğŸ”¹ API anahtarÄ±nÄ±zÄ± yenileyin
                        """
                    else:
                        answer = f"""
                        âŒ **Teknik Hata**
                        
                        Hata: {error_msg[:150]}
                        
                        Genel tavsiyem:
                        ğŸ”¹ Ciddi belirtileriniz varsa doktora baÅŸvurun
                        ğŸ”¹ Acil durumda 112'yi arayÄ±n
                        ğŸ”¹ Sorunuzun kategorisi: {intent}
                        """
        
        # Sonucu gÃ¶ster
        st.markdown(f"**â“ Soru:** {user_input}")
        st.markdown(f"**ğŸ¤– YanÄ±t ({model_choice}):** {answer}")
        
        # Intent bilgisini gÃ¶ster
        if confidence < 0.6 or intent == "non_medical" or (intent not in medical_categories and intent not in ['greeting', 'farewell']):
            st.markdown(f"**ğŸ·ï¸ Intent:** {intent_display}")
        else:
            st.markdown(f"**ğŸ·ï¸ Intent:** {intent} (GÃ¼ven: {confidence:.2f})")
        st.markdown("---")
        
        # GeÃ§miÅŸe ekle
        st.session_state.chat_history.append((user_input, answer, model_choice))

# Model KarÅŸÄ±laÅŸtÄ±rma Analizi
if st.session_state.get("show_comparison_analysis", False) and len(st.session_state.model_comparisons) > 0:
    st.markdown("## ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rma Analizi")
    
    # DataFrame oluÅŸtur
    df_comparisons = pd.DataFrame(st.session_state.model_comparisons)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Toplam KarÅŸÄ±laÅŸtÄ±rma", len(df_comparisons))
    
    with col2:
        avg_confidence = df_comparisons['confidence'].mean()
        st.metric("Ortalama GÃ¼ven", f"{avg_confidence:.3f}")
    
    with col3:
        unique_intents = df_comparisons['intent'].nunique()
        st.metric("FarklÄ± Intent", unique_intents)
    
    # Intent daÄŸÄ±lÄ±mÄ±
    st.subheader("ğŸ·ï¸ Intent DaÄŸÄ±lÄ±mÄ±")
    intent_counts = df_comparisons['intent'].value_counts()
    st.bar_chart(intent_counts)
    
    # Tablo gÃ¶rÃ¼nÃ¼mÃ¼
    st.subheader("ğŸ“‹ KarÅŸÄ±laÅŸtÄ±rma DetaylarÄ±")
    
    for i, row in df_comparisons.iterrows():
        with st.expander(f"Soru {i+1}: {row['question'][:60]}... ({row['intent']})"):
            st.write(f"**Intent:** {row['intent']} (GÃ¼ven: {row['confidence']:.3f})")
            st.write(f"**Zaman:** {row['timestamp']}")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### ğŸ¤– Gemini")
                st.write(row['gemini_answer'])
            
            with col2:
                st.markdown("### ğŸ¦™ Llama")
                st.write(row.get('llama_answer', 'Veri yok'))
    
    # KarÅŸÄ±laÅŸtÄ±rmayÄ± JSON olarak kaydet
    if st.button("ğŸ’¾ KarÅŸÄ±laÅŸtÄ±rmayÄ± JSON'a Kaydet"):
        comparison_file = f"data/model_comparison_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json"
        df_comparisons.to_json(comparison_file, orient='records', indent=2, force_ascii=False)
        st.success(f"âœ… KarÅŸÄ±laÅŸtÄ±rma kaydedildi: {comparison_file}")
    
    if st.button("âŒ Analizi Kapat"):
        st.session_state.show_comparison_analysis = False
        st.rerun()

# Footer
st.markdown("""
---
<div style='text-align: center; color: #666; padding: 20px;'>
    <p>ğŸ¥ TÄ±bbi Asistan Chatbot Â© 2024</p>
    <p><small>âš ï¸ Bu chatbot tÄ±bbi tavsiye vermez. Ciddi durumlar iÃ§in doktora baÅŸvurun.</small></p>
</div>
""", unsafe_allow_html=True)
