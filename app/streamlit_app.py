# Intent SÄ±nÄ±flandÄ±rma (Embedding + LogisticRegression)
import streamlit as st
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
import joblib

from dotenv import load_dotenv
import os

# LangChain + Gemini + Llama
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# BaÅŸlÄ±k
st.set_page_config(page_title="Ubuntu Help Chatbot", layout="wide")

# Sol Ã¼st kÃ¶ÅŸe baÅŸlÄ±k 
st.sidebar.markdown("""
    <style>
    .sidebar-title {
        font-size: 20px;
        font-weight: bold;
        padding: 10px 0 5px 5px;
        color: #ffffff;
    }

    section[data-testid="stSidebar"]::before {
        content: " ";
        display: block;
        margin-bottom: 10px;
        border-bottom: 1px solid #444;
    }
    </style>
    <div class="sidebar-title"> ğŸ’» Ubuntu Helpdesk</div>
""", unsafe_allow_html=True)

# Session state
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "model_comparisons" not in st.session_state:
    st.session_state.model_comparisons = []

if "show_comparison_analysis" not in st.session_state:
    st.session_state.show_comparison_analysis = False

import os
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Intent Model ve Label Encoder YÃ¼kleme
model_path = os.path.join(BASE_DIR, "..", "data", "intent_classifier.joblib")
encoder_path = os.path.join(BASE_DIR, "..", "data", "label_encoder.joblib")

if not os.path.exists(model_path):
    try:
        st.info("ğŸ”„  Model file not found, training a new model...")
        
        # Dataset dosyasÄ±nÄ±n varlÄ±ÄŸÄ±nÄ± kontrol et
        dataset_path = os.path.join(BASE_DIR, "..", "ubuntu_chatbot_dataset.xlsx")
        if not os.path.exists(dataset_path):
            st.error(f"âŒ Dataset file not found: {dataset_path}")
            st.stop()
        
        # Data klasÃ¶rÃ¼nÃ¼ oluÅŸtur (yoksa)
        data_dir = os.path.join(BASE_DIR, "..", "data")
        os.makedirs(data_dir, exist_ok=True)
        
        # TamamlanmÄ±ÅŸ dataset'i yÃ¼kle
        st.write(f"ğŸ“‚ Loading dataset: {dataset_path}")
        df = pd.read_excel(dataset_path)
        df = df.dropna()
        
        if df.empty:
            st.error("âŒ Dataset is empty!")
            st.stop()
        
        # SÃ¼tun kontrolÃ¼
        required_columns = ['user_message', 'balanced_intent']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            st.error(f"âŒ Missing columns on dataset: {missing_columns}")
            st.write("Existing Columns", list(df.columns))
            st.stop()
        
        # Dengeli veri setinde balanced_intent kullan
        st.write(f"ğŸ“Š Dataset loaded: {len(df)} Ã¶rnek")
        st.write(f"ğŸ·ï¸ Number of categories: {len(df['balanced_intent'].unique())} adet")
        
        # Dengeli intent'leri kullan
        df['simplified_intent'] = df['balanced_intent']
        
        # Intent daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
        intent_counts = df['simplified_intent'].value_counts()
        st.write("ğŸ“ˆ Dataset Distribution:")
        st.dataframe(intent_counts)

        # Embedding modeli yÃ¼kle
        st.write("ğŸ¤– Preparing embedding model for training...")
        import torch
        
        # Device belirleme
        if torch.backends.mps.is_available():
            device = "mps"
        elif torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
        
        try:
            embedding_model_training = SentenceTransformer(
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                device=device
            )
        except:
            st.warning("âš ï¸ Main model could not be loaded, using alternative model...")
            embedding_model_training = SentenceTransformer(
                "sentence-transformers/all-MiniLM-L6-v2",
                device="cpu"
            )
        
        # Embedding oluÅŸtur
        st.write("ğŸ”§ Generating embeddings...")
        X = embedding_model_training.encode(df["user_message"].tolist())
        
        # Label encoder
        st.write("ğŸ·ï¸ Label encoding...")
        le = LabelEncoder()
        y = le.fit_transform(df["simplified_intent"])

        # Model EÄŸitim
        st.write("ğŸ‹ï¸ Training the model...")
        clf = LogisticRegression(max_iter=1000, random_state=42, C=1.0, class_weight='balanced')
        clf.fit(X, y)
        
        # Model performansÄ±nÄ± test et
        from sklearn.model_selection import cross_val_score
        scores = cross_val_score(clf, X, y, cv=5)
        st.success(f"âœ… Model trained! Accuracy: {scores.mean():.3f}")

        # Kaydet
        st.write("ğŸ’¾ Saving the model...")
        joblib.dump(clf, model_path)
        joblib.dump(le, encoder_path)
        st.success(f"âœ… Model saved successfully!")
        st.success(f"ğŸ“ Model folder: {model_path}")
        st.success(f"ğŸ“ Encoder folder: {encoder_path}")
        
    except Exception as e:
        st.error(f"âŒ Error during model training: {str(e)}")
        st.error(f"Error Type: {type(e).__name__}")
        import traceback
        st.code(traceback.format_exc())
        st.stop()
else:
    try:
        # Mevcut modeli yÃ¼kle
        clf = joblib.load(model_path)
        le = joblib.load(encoder_path)
        st.success("ğŸ¤– Existing model loaded successfully!")
    except Exception as e:
        st.error(f"âŒ Error while loading the model: {str(e)}")
        st.stop()

# Embedding modeli gÃ¼venli yÃ¼kleme
@st.cache_resource
def load_embedding_model():
    import torch
    try:
        # Device belirleme (Apple Silicon Mac iÃ§in)
        if torch.backends.mps.is_available():
            device = "mps"
        elif torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
        
        st.info(f"ğŸ¤– Embedding model is loading... (Device: {device})")
        
        # TÃ¼rkÃ§e destekli model - device belirtme
        model = SentenceTransformer(
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            device=device
        )
        st.success("âœ… Embedding loaded successfully!")
        return model
        
    except Exception as e:
        st.warning(f"âš ï¸ Can't load the main model: {e}")
        st.info("ğŸ”„ Trying for alternate model...")
        
        # Fallback: Daha basit model
        try:
            model = SentenceTransformer(
                "sentence-transformers/all-MiniLM-L6-v2",
                device="cpu"  # CPU'da Ã§alÄ±ÅŸtÄ±r
            )
            st.success("âœ…  Alternative embedding model loaded!")
            return model
        except Exception as e2:
            st.error(f"ğŸš« No embedding model could be loaded: {e2}")
            return None

# Global embedding model
embedding_model = load_embedding_model()

# Intent Tahmin Fonksiyonu (GÃ¼Ã§lendirilmiÅŸ)
def predict_intent(text):
    if embedding_model is None:
        # Fallback: gÃ¼venilir deÄŸil
        return "not_related", 0.1
    
    try:
        # Metni encode et
        vec = embedding_model.encode([text])
        
        # Tahmin yap
        pred = clf.predict(vec)
        probabilities = clf.predict_proba(vec)[0]
        confidence = probabilities.max()
        intent = le.inverse_transform(pred)[0]
        
        # En yÃ¼ksek 2 probability'yi karÅŸÄ±laÅŸtÄ±r (belirsizlik kontrolÃ¼)
        sorted_probs = sorted(probabilities, reverse=True)
        if len(sorted_probs) > 1:
            uncertainty = sorted_probs[0] - sorted_probs[1]
            # EÄŸer en yÃ¼ksek ile ikinci yÃ¼ksek arasÄ±nda az fark varsa gÃ¼ven azalt
            if uncertainty < 0.2:
                confidence *= 0.8
        
        return intent, confidence
        
    except Exception as e:
        print(f"Intent tahmin hatasÄ±: {e}")
        return "not_related", 0.1

# LLM Modeller
llm_gemini = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash-latest",
    temperature=0.3,
    max_tokens=500,
    google_api_key="AIzaSyDuqyG98CCFaeoBo8zVfffyGR0XmQDyXtY",
    convert_system_message_to_human=True
)

# Llama Model (Local - Ollama)
llm_llama = ChatOllama(
    model="llama3.2:3b",
    temperature=0.3,
    num_predict=500,
)

# Prompt
system_prompt = (
    "You are an experienced technical assistant specializing in Ubuntu operating system support. You help users solve Ubuntu-related issues in a clear and user-friendly manner."

    "IMPORTANT GUIDELINES:"

    "1. Do NOT execute system commands or make changes directly â€” only explain steps clearly."
    "2. If the issue is critical (e.g., system crash or boot failure), advise the user to consult an experienced technician or support forum."
    "3. Do NOT recommend or install third-party scripts or software unless they are officially trusted."
    "4. Always keep your answers concise, beginner-friendly, and structured step-by-step."
    "5. When needed, explain terminal commands and their effects briefly."

    "Your role is to support users with Ubuntu desktop and server problems, including package issues, system settings, permissions, updates, and basic troubleshooting."
    "Content given: {context}"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}")
])

# Zincir
qa_chain_gemini = create_stuff_documents_chain(llm_gemini, prompt)

# RAG zinciri
def run_rag_chain(question, model_choice):
    # Model seÃ§imine gÃ¶re LLM belirle
    if "Llama" in model_choice:
        llm = llm_llama
        qa_chain = create_stuff_documents_chain(llm, prompt)
    else:  # Gemini
        llm = llm_gemini
        qa_chain = qa_chain_gemini
    
        if "Llama" in model_choice:
            # Llama iÃ§in sistem promptu dahil etme
            full_prompt = f"""You are an experienced technical assistant specializing in Ubuntu operating system support. You help users solve Ubuntu-related issues in a clear and user-friendly manner.

IMPORTANT GUIDELINES:

1. Do NOT execute system commands or make changes directly â€” only explain steps clearly.
2. If the issue is critical (e.g., system crash or boot failure), advise the user to consult an experienced technician or support forum.
3. Do NOT recommend or install third-party scripts or software unless they are officially trusted.
4. Always keep your answers concise, beginner-friendly, and structured step-by-step.
5. When needed, explain terminal commands and their effects briefly.

Question: {question}

Answer:"""
            response = llm.invoke(full_prompt)
            return response.content
        else:  # Gemini
            return llm.invoke(question).content

# Sidebar ayarlarÄ±
with st.sidebar:
    st.header("âš™ï¸ Settings")
    model_choice = st.selectbox("ğŸ¤– Model Selection:", [
        "Gemini-1.5-flash", 
        "Llama-3.2-3B (Local)",
        "Compare (Both)"
    ])
    
    # Model karÅŸÄ±laÅŸtÄ±rma Ã¶zelliÄŸi
    if model_choice == "Compare (Both)":
        st.info("ğŸ”„ Gemini and Llama models are going to compare!")
    elif model_choice == "Llama-3.2-3B (Local)":
        st.success("ğŸ¦™ Local Llama model is being use - No API key needed!")
    
    
    if st.button("ğŸ—‘ï¸ Clear History"):
        st.session_state.chat_history = []
        st.rerun()
    
    # Model karÅŸÄ±laÅŸtÄ±rmasÄ± geÃ§miÅŸi
    if len(st.session_state.model_comparisons) > 0:
        st.markdown("---")
        st.subheader("ğŸ“Š Model Comparison")
        st.write(f"Total Tomparison: {len(st.session_state.model_comparisons)}")
        
        if st.button("ğŸ“ˆ Comparison Analysis"):
            st.session_state.show_comparison_analysis = True
        
        if st.button("ğŸ—‘ï¸ Clear the comparisons"):
            st.session_state.model_comparisons = []
            st.rerun()

# Ana baÅŸlÄ±k
st.title("ğŸ’» Ubuntu Help Chatbot")
st.markdown("*Ask anything about Ubuntu*")

# GeÃ§miÅŸi gÃ¶ster
if st.session_state.chat_history:
    st.markdown("## ğŸ’¬ Chat History")
    for i, (q, a, m) in enumerate(st.session_state.chat_history):
        st.markdown(f"**Question {i+1}:** {q}")
        st.markdown(f"**Answer ({m}):** {a}")
        st.markdown("---")

# KullanÄ±cÄ± input
user_input = st.text_input("ğŸ’­ Ask your question:", placeholder="Example: How to update apt?")
st.button("ğŸ“¤ Send", use_container_width=True)

if user_input:
    with st.spinner("ğŸ¤” Thinking..."):
        # Intent tahmin et
        intent, confidence = predict_intent(user_input)
        
        # Sadece doÄŸru kategoriler tanÄ±mla (greeting ve farewell hariÃ§)
        ubuntu_categories = {
            "networking","terminal_usage","system_configuration","file_operations","package_management","user_management","process_management","disk_and_partition","security",'other'
        }
        
        # SIKI Confidence threshold kontrolÃ¼ ve kategori filtreleme
        if confidence < 0.6 or intent == "not_related" or (intent not in ubuntu_categories and intent not in ['greeting', 'farewell']):
            answer = f"""
            I can only assist you with the following topics related to Ubuntu and Linux systems:

            ğŸ§  My Expertise Areas:

            ğŸ’» General Ubuntu support (installations, updates, package issues)
            ğŸ”§ Terminal commands and usage (bash, apt, dpkg, etc.)
            ğŸ“¦ Software and package management
            ğŸ§± System performance and monitoring
            ğŸŒ Network configuration and troubleshooting
            ğŸ‘¤ User and permission management
            ğŸ›¡ï¸ Security and firewall settings
            ğŸ¨ Desktop environment issues (GNOME, KDE, XFCE...)
            ğŸ“ File system and storage (mount, disk, partitions)
            ğŸ§ Linux kernel and modules
            ğŸ’¡ Tips and best practices for Ubuntu users
            ğŸ“¥ Dual boot and virtualization issues
            ğŸ“¦ Snap, Flatpak, AppImage usage
            ğŸ§© Troubleshooting installation errors
            âš™ï¸ System services and daemons (systemd, cron, etc.)

             Please ask a question related to one of the topics above.

            ğŸ”„ If you're unsure where to start, try asking something like:
            â€œHow can I install a .deb file on Ubuntu?â€
                or
            â€œWhy is my Wi-Fi not working on Ubuntu 22.04?â€


            """
            intent_display = f"Out of Scope - Intent: {intent} (Confidence: {confidence:.2f})"
        
        # EÄŸer intent konu dÄ±ÅŸÄ± ise sabit cevap 
        elif intent == "greeting":
            greetings = [
                "Hello! How can I assist you with Ubuntu today? ğŸ§",
                "Hi there! Feel free to ask any Ubuntu-related questions. ğŸ’»",
                "Welcome! I'm here to help you with your Ubuntu system. ğŸ˜Š",
                "Greetings! Need help with commands or troubleshooting in Ubuntu? ğŸ”§",
                "Hey! Iâ€™m your Ubuntu assistant bot. Ask me anything. ğŸ¤–"
            ]
            import random
            answer = random.choice(greetings)
        # Farewell (VedalaÅŸma) mesajlarÄ±  
        elif intent == "farewell":
            farewells = [
                "See you later! Happy Ubuntu-ing! ğŸ§¡",
                "Goodbye! Feel free to return with more questions. ğŸ‘‹",
                "Take care! Wishing you smooth Linux experiences. ğŸ§",
                "Bye! I'm always here if you need Ubuntu support again. ğŸ’¬",
                "Farewell! Don't forget to keep your system updated. ğŸ”„"
            ]
            import random
            answer = random.choice(farewells)

        else:
            # Normal sorular - Model seÃ§imine gÃ¶re Ã§alÄ±ÅŸtÄ±r
            if model_choice == "Comprasion (Both)":
                # Her iki modeli de Ã§alÄ±ÅŸtÄ±r
                answers = {}
                models_to_test = ["Gemini-1.5-flash", "Llama-3.2-3B (Local)"]
                
                for model in models_to_test:
                    try:
                        current_answer = run_rag_chain(user_input, model)
                        
                        # EÄŸer Ã§ok kÄ±sa cevap geliyorsa, detaylandÄ±r
                        if len(current_answer) < 100:
                            enhanced_prompt = f"""
                            User's Ubuntu question: "{user_input}"
                            Detected category: {intent}

                            Provide a detailed, helpful, and user-friendly answer to this Ubuntu-related issue.
                            Include relevant terminal commands if necessary, explain key steps clearly,
                            and remind the user to take precautions such as backing up data before making major changes.
                            """
                            if "Llama" in model:
                                current_answer = llm_llama.invoke(enhanced_prompt).content
                            else:
                                current_answer = llm_gemini.invoke(enhanced_prompt).content
                        
                        # GÃ¼venlik uyarÄ±sÄ± ekle
                        current_answer += "\n\nâš ï¸ *Remember to back up your data before applying critical system changes. Use terminal commands carefully.*"
                        answers[model] = current_answer
                        
                    except Exception as e:
                        error_msg = str(e)
                        if "API key" in error_msg.lower() or "Unauthorized" in error_msg.lower():
                            answers[model] = f"âŒ API Key Error: {model} invalid or missing API Key."
                        else:
                            answers[model] = f"âŒ Error: {model} - {error_msg[:100]}"
                
                # KarÅŸÄ±laÅŸtÄ±rma sonucunu kaydet
                comparison_data = {
                    "timestamp": pd.Timestamp.now(),
                    "question": user_input,
                    "intent": intent,
                    "confidence": confidence,
                    "gemini_answer": answers.get("Gemini-1.5-flash", "Hata"),
                    "llama_answer": answers.get("Llama-3.2-3B (Local)", "Hata")
                }
                st.session_state.model_comparisons.append(comparison_data)
                
                # KarÅŸÄ±laÅŸtÄ±rmalÄ± yanÄ±tÄ± hazÄ±rla
                answer = f"""
                ## ğŸ¤– **Gemini-1.5-flash Response:**
                {answers.get('Gemini-1.5-flash', 'Hata oluÅŸtu')}
                
                ---
                
                ## ğŸ¦™ **Llama-3.2-3B (Local) Response:**
                {answers.get('Llama-3.2-3B (Local)', 'Hata oluÅŸtu')}
                """
                
            else:
                # Tek model Ã§alÄ±ÅŸtÄ±r
                try:
                    answer = run_rag_chain(user_input, model_choice)
                    
                    # EÄŸer Ã§ok kÄ±sa cevap geliyorsa, detaylandÄ±r
                    if len(answer) < 100:
                        enhanced_prompt = f"""
                            User's Ubuntu question: "{user_input}"
                            Detected category: {intent}
                            
                            Provide a detailed, helpful, and user-friendly answer to this Ubuntu-related issue.
                            Include relevant terminal commands if necessary, explain key steps clearly,
                            and remind the user to take precautions such as backing up data before making major changes.
                            """
                        if "Llama" in model_choice:
                            answer = llm_llama.invoke(enhanced_prompt).content
                        else:
                            answer = llm_gemini.invoke(enhanced_prompt).content
                    
                    # GÃ¼venlik uyarÄ±sÄ± ekle
                    answer += "\n\nâš ï¸ *Remember to back up your data before applying critical system changes. Use terminal commands carefully.*"
                    
                except Exception as e:
                    error_msg = str(e)
                    if "API key" in error_msg.lower() or "unauthorized" in error_msg.lower():
                        answer = f"""
                        The API key for the {model_choice} model is invalid or missing.
                        Please check your API key in the .env file.
    
                        Temporary solutions:
                        ğŸ”¹ Select a different model
                        ğŸ”¹ Renew your API key
                        """
                    else:
                        answer = f"""
                        âŒ **Technical Error**
                        
                        Error: {error_msg[:150]}
                        
                        My general advice:
                        ğŸ”¹ If you have severe symptoms, please consult a doctor
                        ğŸ”¹ In case of emergency, call your local emergency number (e.g., 112)
                        ğŸ”¹ Detected category of your question: {intent}
                        """
        # Sonucu gÃ¶ster
        st.markdown(f"**â“ Question** {user_input}")
        st.markdown(f"**ğŸ¤– Answer ({model_choice}):** {answer}")
        
        # Intent bilgisini gÃ¶ster
        if confidence < 0.6 or intent == "not_related" or (intent not in ubuntu_categories and intent not in ['greeting', 'farewell']):
            st.markdown(f"**ğŸ·ï¸ Intent:** {intent_display}")
        else:
            st.markdown(f"**ğŸ·ï¸ Intent:** {intent} (Confidence: {confidence:.2f})")
        st.markdown("---")
        
        # GeÃ§miÅŸe ekle
        st.session_state.chat_history.append((user_input, answer, model_choice))

# Model KarÅŸÄ±laÅŸtÄ±rma Analizi
if st.session_state.get("show_comparison_analysis", False) and len(st.session_state.model_comparisons) > 0:
    st.markdown("## ğŸ“Š Model Cpmrasion Anaylsis")
    
    # DataFrame oluÅŸtur
    df_comparisons = pd.DataFrame(st.session_state.model_comparisons)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Total Comprasion", len(df_comparisons))
    
    with col2:
        avg_confidence = df_comparisons['confidence'].mean()
        st.metric("Average Confidence", f"{avg_confidence:.3f}")
    
    with col3:
        unique_intents = df_comparisons['intent'].nunique()
        st.metric("Different Intent", unique_intents)
    
    # Intent daÄŸÄ±lÄ±mÄ±
    st.subheader("ğŸ·ï¸ Intent Distribution")
    intent_counts = df_comparisons['intent'].value_counts()
    st.bar_chart(intent_counts)
    
    # Tablo gÃ¶rÃ¼nÃ¼mÃ¼
    st.subheader("ğŸ“‹ Comprasion Details")
    
    for i, row in df_comparisons.iterrows():
        with st.expander(f"Soru {i+1}: {row['question'][:60]}... ({row['intent']})"):
            st.write(f"**Intent:** {row['intent']} (Confidence: {row['confidence']:.3f})")
            st.write(f"**Time:** {row['timestamp']}")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### ğŸ¤– Gemini")
                st.write(row['gemini_answer'])
            
            with col2:
                st.markdown("### ğŸ¦™ Llama")
                st.write(row.get('llama_answer', 'No Data'))
    
    # KarÅŸÄ±laÅŸtÄ±rmayÄ± JSON olarak kaydet
    if st.button("ğŸ’¾ Save the comprasion to JSON"):
        comparison_file = f"data/model_comparison_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json"
        df_comparisons.to_json(comparison_file, orient='records', indent=2, force_ascii=False)
        st.success(f"âœ… Comprasion Saved: {comparison_file}")
    
    if st.button("âŒ Close Analysis"):
        st.session_state.show_comparison_analysis = False
        st.rerun()

# Footer
st.markdown("""
---
<div style='text-align: center; color: #666; padding: 20px;'>
    <p>ğŸ’» Ubuntu Chatbot Â© 2025</p>
    <p><small>âš ï¸ Please backup your data before any process </small></p>
</div>
""", unsafe_allow_html=True)
